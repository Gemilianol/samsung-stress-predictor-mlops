{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf20075",
   "metadata": {},
   "source": [
    "# üìÅ 03 - Feature Selection\n",
    "\n",
    "### üéØ Objective\n",
    "Since this time series problem can also be approached as a tabular regression task, the goal of this notebook is to identify which features contribute the most to the prediction of the target variable (`stress_score`) by evaluating their relevance and statistical significance.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why Tabular First?\n",
    "Although this is fundamentally a time series problem, treating it as a tabular problem allows us to:\n",
    "- Detect useful **patterns and relationships**\n",
    "- Quickly evaluate **feature importance** via models like Random Forest or XGBoost\n",
    "- Test **lagged variables** and their predictive power\n",
    "- Analyze **multicollinearity** (e.g., via VIF)\n",
    "- Use **OLS regression** to inspect coefficients, p-values, and R¬≤\n",
    "\n",
    "This helps us to **narrow down the feature space** before moving into deep learning or time-series-specific models (e.g., LSTM, ARIMA).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Key Steps\n",
    "\n",
    "1. **Train Tree-Based Models**  \n",
    "   - Use Random Forest to compute feature importances\n",
    "   - Test parameter tuning with `RandomizedSearchCV`\n",
    "\n",
    "2. **Lag Analysis**  \n",
    "   - Test lagged versions (1‚Äì5 days) of the most important features  \n",
    "   - Observe performance changes and feature importance evolution\n",
    "\n",
    "3. **OLS Regression & Multicollinearity**  \n",
    "   - Fit a Multiple Linear Regression model  \n",
    "   - Use p-values and adjusted R¬≤ for insight  \n",
    "   - Analyze VIF to detect multicollinearity\n",
    "\n",
    "4. **Final Selection**  \n",
    "   - Keep the most informative features  \n",
    "   - Prepare a lightweight dataset to be passed into the modeling stage\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Output\n",
    "- `data-for-model.csv`: A reduced dataset containing only the features that showed predictive power and low multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "> üìù Note: This notebook acts as a bridge between EDA and model training. The idea is to simplify the dataset for better model performance and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd23ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "#Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "#Supress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed/data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c07ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic train test split based on date:\n",
    "x_train = data[data[\"date\"] < \"2025-01-01\"]\n",
    "y_train = x_train['stress_score']\n",
    "# Here we need to drop stress_score becuase it's the target and the date becuase if we treat as tabular data doesn't matter:\n",
    "x_train.drop(columns=['stress_score', 'date'], inplace=True)\n",
    "x_test = data[data[\"date\"] >= \"2025-01-01\"]\n",
    "y_test = x_test['stress_score']\n",
    "# Same here:\n",
    "x_test.drop(columns=['stress_score', 'date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we can try a RF as default so:\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "preds = rf.predict(x_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(f'RMSE of Random Forest Regressor (Default): {np.round(rmse, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.Series(rf.feature_importances_, index=rf.feature_names_in_)\n",
    "features.sort_values(ascending=False).head(10).plot(kind='barh', figsize=(10, 8))\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afa3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can try tunning Hyperparameters through Random Search CV:\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "params = {'n_estimators': [10, 50, 75, 100, 150],\n",
    "          'max_depth': [2, 4, 6, 8],\n",
    "          'min_samples_split': [2, 4, 6, 8],\n",
    "          'min_samples_leaf': [1, 2, 3, 4],\n",
    "          'max_features': [0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1], #1 means all features   \n",
    "}\n",
    "\n",
    "rf_best = RandomizedSearchCV(estimator=rf, \n",
    "                             param_distributions=params,\n",
    "                             n_iter=10,\n",
    "                             scoring='neg_root_mean_squared_error',\n",
    "                             n_jobs=-1,\n",
    "                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf_best.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(f'RMSE of Random Forest Regressor (Tuned):: {np.round(rmse, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you run RandomizedSearchCV, the resulting object rf_best is not the trained model itself ‚Äî it's a wrapper around it.\n",
    "rf_best = rf_best.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef58b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.Series(rf_best.feature_importances_, index=rf_best.feature_names_in_)\n",
    "features.sort_values(ascending=False).head(10).plot(kind='barh', figsize=(10, 8))\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can keep the five more important features (RF with less RMSE) and lag them in order to see if we improve the performance.\n",
    "# Also we need to keep the date in order to do the train test split and the target too. \n",
    "\n",
    "data = data[['date', 'stress_max', 'stress_min',\n",
    "             'heart_rate', 'heart_min_rate',\n",
    "             'heart_max_rate', 'stress_score'\n",
    "             ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stress_max_lag1'] = data['stress_max'].shift(1)\n",
    "data['stress_max_lag2'] = data['stress_max'].shift(2)\n",
    "data['stress_max_lag3'] = data['stress_max'].shift(3)\n",
    "data['stress_max_lag4'] = data['stress_max'].shift(4)\n",
    "data['stress_max_lag5'] = data['stress_max'].shift(5)\n",
    "\n",
    "data['stress_min_lag1'] = data['stress_min'].shift(1)\n",
    "data['stress_min_lag2'] = data['stress_min'].shift(2)\n",
    "data['stress_min_lag3'] = data['stress_min'].shift(3)\n",
    "data['stress_min_lag4'] = data['stress_min'].shift(4)\n",
    "data['stress_min_lag5'] = data['stress_min'].shift(5)\n",
    "\n",
    "data['heart_rate_lag1'] = data['heart_rate'].shift(1)\n",
    "data['heart_rate_lag2'] = data['heart_rate'].shift(2)\n",
    "data['heart_rate_lag3'] = data['heart_rate'].shift(3)\n",
    "data['heart_rate_lag4'] = data['heart_rate'].shift(4)\n",
    "data['heart_rate_lag5'] = data['heart_rate'].shift(5)\n",
    "\n",
    "data['heart_min_rate_lag1'] = data['heart_min_rate'].shift(1)\n",
    "data['heart_min_rate_lag2'] = data['heart_min_rate'].shift(2)\n",
    "data['heart_min_rate_lag3'] = data['heart_min_rate'].shift(3)\n",
    "data['heart_min_rate_lag4'] = data['heart_min_rate'].shift(4)\n",
    "data['heart_min_rate_lag5'] = data['heart_min_rate'].shift(5)\n",
    "\n",
    "data['heart_max_rate_lag1'] = data['heart_max_rate'].shift(1)\n",
    "data['heart_max_rate_lag2'] = data['heart_max_rate'].shift(2)\n",
    "data['heart_max_rate_lag3'] = data['heart_max_rate'].shift(3)\n",
    "data['heart_max_rate_lag4'] = data['heart_max_rate'].shift(4)\n",
    "data['heart_max_rate_lag5'] = data['heart_max_rate'].shift(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NaN *after* all shifts\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072cad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cab711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5dc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same train test split as before with this 'new' dataset:\n",
    "\n",
    "x_train = data[data[\"date\"] < \"2025-01-01\"]\n",
    "y_train = x_train['stress_score']\n",
    "x_train.drop(columns=['stress_score', 'date'], inplace=True)\n",
    "\n",
    "x_test = data[data[\"date\"] >= \"2025-01-01\"]\n",
    "y_test = x_test['stress_score']\n",
    "x_test.drop(columns=['stress_score','date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A default RF I think it's enough to compare with the preview one:\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "preds = rf.predict(x_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(f'RMSE of Random Forest Regressor (Default): {np.round(rmse, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea815ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.Series(rf.feature_importances_, index=rf.feature_names_in_)\n",
    "features.sort_values(ascending=False).head(10).plot(kind='barh', figsize=(10, 8))\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a082486",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c37429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statsmodel is more clearly for statistical representation than Sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Use same train-test split\n",
    "X_train_lr = x_train.copy()\n",
    "X_test_lr = x_test.copy()\n",
    "\n",
    "# Add constant for statsmodels\n",
    "X_train_sm = sm.add_constant(X_train_lr)\n",
    "model = sm.OLS(y_train, X_train_sm).fit()\n",
    "print(model.summary())  # this shows adjusted R2, p-values, confidence intervals\n",
    "\n",
    "# Predict and get RMSE\n",
    "X_test_sm = sm.add_constant(X_test_lr)\n",
    "y_pred = model.predict(X_test_sm)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c23e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# X: your features (without the target), and including the constant\n",
    "X = sm.add_constant(x_train)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['feature'] = X.columns\n",
    "# Parameters exog{ndarray, DataFrame} and exog_idx int => index of the exogenous variable in the columns of exog\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif.sort_values(by='VIF', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68516aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reload the data in order to recuperate the rows missed on shifted procedure \n",
    "# and do it again only for 3 lags and at the end save it with the features selected:\n",
    "\n",
    "data = pd.read_csv('../data/processed/data.csv', sep=',', \n",
    "                   usecols=['date', 'stress_max', 'stress_min',\n",
    "             'heart_rate', 'heart_min_rate',\n",
    "             'heart_max_rate', 'stress_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c86eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72910bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['heart_min_rate_lag1'] = data['heart_min_rate'].shift(1)\n",
    "data['heart_min_rate_lag2'] = data['heart_min_rate'].shift(2)\n",
    "data['heart_min_rate_lag3'] = data['heart_min_rate'].shift(3)\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1feca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/processed/data-for-model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa55a09",
   "metadata": {},
   "source": [
    "## üìå Insights Summary\n",
    "\n",
    "- **Stress-related features dominate prediction:**  \n",
    "  `stress_max`, `stress_min`, and `heart_rate` show the strongest correlation with `stress_score`.\n",
    "\n",
    "- **Lagged features matter:**  \n",
    "  Adding lags (1‚Äì3 days) of min heart rate improved RMSE significantly in Random Forest.\n",
    "\n",
    "- **Sleep metrics underperformed:**  \n",
    "  No strong predictive power from `sleep_score`, `sleep_efficiency`, or recovery indicators, possibly due to missing data or weak relationship.\n",
    "\n",
    "- **Multicollinearity warning:**  \n",
    "  VIF analysis revealed redundancy among lagged `heart_rate` variables ‚Äî need for dimensionality control.\n",
    "\n",
    "These findings informed the choice to proceed with time series‚Äìoriented models and reduced feature sets for better generalization and interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
